# ch3 贝叶斯学习

## 1. 贝叶斯学习的背景

$$P(h | D) = \frac{P(D|h)P(h)}{P(D)}$$

- $P(h|D)$：$h$ 的后验概率
- $P(h)$：假设 $h$ 的先验概率，假设空间 $H$ 中的数据必须完备且互斥，$\sum P(h_i) = 1$
- $P(D)$：数据集 $D$ 的先验概率，$D$ 是所有可能数据的样本，和假设是独立的，在比较不同假设时可以忽略
- $P(D|h)$：给定 $h$ 发生的情况下，$D$ 的概率，即似然度（$\tt likelihood$）

一般来说，我们希望在给定训练数据的情况下得到最可能的假设，称为**极大后验假设**：

$h_{MAP} = \arg\max_{h \in H}(P(h|D)) = \arg\max_{h \in H}(P(D|h)P(h))$

## 2. 极大似然假设

- 极大似然假设：$h_{ML} = \arg\max_{h_i \in H}(P(D|h_i)) =\arg\max_{h_i \in H} \sum_{i=1}^{m}(d_i - h(x_i))^2$

如果我们完全不知道假设的概率分布，或者我们知道所有的假设发生的概率相同，那么极大后验假设等价于极大似然假设。

## 3. 朴素贝叶斯分类器

- 朴素贝叶斯假设：$P(x|v_j) = P(a_1, a_2\dots a_n|v_j) = \Pi_i P(a_i|v_j)$
- 朴素贝叶斯分类器：$v_{NB} =\arg\max_{v_j \in V} P(v_j) \Pi_i P(a_i|v_j) = \arg\max_{v_j \in V}\{\log P(v_j + \sum_i \log P(a_i|v_j)\} $

## 4. 最小描述长度

更倾向于最小化的假设 $h$：$h_{MDL} = \arg\max_{h \in H}\{L_{C_1}(h) + L_{C_2}(D|h)\}$，其中 $L_C(x)$ 是编码 $C$ 下 $x$ 的描述长度。